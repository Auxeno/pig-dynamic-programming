{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Solving the Dice Game Pig with Dynamic Programming\n*This notebook is going to be updated frequently, stay tuned*.\n\nv2 Changes:\n- get_observations now returns observations as a tuple so it can be used as a key for a dictionary\n- Implemented a simple 'Hold at 20' agent that can interact with the environment.\n- Computed the state-value function for the hold at 20 strategy vs the same hold at 20 strategy.","metadata":{}},{"cell_type":"code","source":"import random\n\nclass Pig:\n    def __init__(self):\n        self.state = [0, 0, 0]\n        self.current_player = 1\n        self.player_names = (\"Player 1\", \"Player 2\")\n        self.history = []\n        \n    def roll(self):\n        dice_value = random.randint(1, 6)\n        if dice_value == 1:\n            self.state[2] = 0\n        else:\n            self.state[2] += dice_value\n        return dice_value\n        \n    def hold(self):\n        self.state[0] += self.state[2]\n        self.state[2] = 0\n        return 0  # 0 indicates hold\n    \n    def change_player(self):\n        self.state[0], self.state[1] = self.state[1], self.state[0]\n        self.current_player = 3 - self.current_player\n    \n    def action(self, action):\n        assert action in [0, 1], \"Invalid move selected.\"  \n        assert not self.is_done(), \"Game is over. Actions not possible.\"\n            \n        if action == 0:\n            roll_value = self.hold()\n        elif action == 1:\n            roll_value = self.roll()\n            \n        info = self.get_info(action, roll_value)\n        \n        if roll_value in [0, 1]:  # if hold or roll a 1\n            if self.state[0] < 100:\n                self.change_player()\n        \n        observations = self.get_observations()\n        reward = self.calculate_reward()\n        done = self.is_done()\n        \n        return observations, reward, done, info\n        \n    def get_observations(self):\n        return tuple(self.state.copy())\n    \n    def get_actions(self):\n        return [] if self.is_done() else [0, 1]\n            \n    def reset(self):\n        self.state = [0, 0, 0]\n        self.current_player = 1\n        return self.get_observations()\n    \n    def calculate_reward(self):\n        if self.state[0] >= 100:\n            return 1\n        elif self.state[1] >= 100:\n            return -1\n        else:\n            return 0\n        \n    def get_info(self, action, roll_value):\n        info = self.player_names[self.current_player-1]\n        if action == 0:\n            info += f\" holds, bringing their total score to {self.state[0]}.\"\n        elif action == 1 and roll_value != 1:\n            info += f\" rolls a {roll_value}, bringing their turn score to {self.state[2]} and total score to {self.state[0] + self.state[2]}.\"\n        elif action == 1 and roll_value == 1:\n            info += f\" rolls a 1, losing their turn score, resetting their total score to {self.state[0] + self.state[2]}.\"\n        return info\n        \n    def is_done(self):\n        return self.state[0] >= 100 or self.state[1] >= 100\n    \n    \nfrom collections import defaultdict\n\nclass HoldOn20Agent:\n    def __init__(self):\n        self.reward = 0\n        self.policy = defaultdict(int)  # Initialize policy dictionary\n        \n        # Populate policy dictionary with all possible states for plotting\n        for p_score in range(0, 106):\n            for o_score in range(0, 106):\n                for t_total in range(0, 106):\n                    if p_score + t_total  >= 100:\n                        self.policy[(p_score, o_score, t_total)] = 0 # Hold and win\n                    if t_total >= 20:\n                        self.policy[(p_score, o_score, t_total)] = 0 # Hold\n                    else:\n                        self.policy[(p_score, o_score, t_total)] = 1 # Roll\n                        \n    def reset(self):\n        self.reward = 0\n\n    def step(self, env: Pig):\n        # Get observations from environment to make decision with (discarded with random agent)\n        state = env.get_observations()\n\n        # Get available actions and choose a random one\n        actions = env.get_actions()\n\n        action = self.policy[state]\n\n        obs, reward, done, info = env.action(action)\n        self.reward += reward","metadata":{"execution":{"iopub.status.busy":"2023-06-17T12:48:27.883503Z","iopub.execute_input":"2023-06-17T12:48:27.884148Z","iopub.status.idle":"2023-06-17T12:48:27.899465Z","shell.execute_reply.started":"2023-06-17T12:48:27.884114Z","shell.execute_reply":"2023-06-17T12:48:27.898221Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Modelling this as a Reinforcement Learning Problem\n\nThe game of Pig lends itself naturally to the framework of reinforcement learning. In reinforcement learning, an agent (the player in our case) interacts with an environment (the game of Pig), receiving feedback in terms of rewards (winning or losing the game) and tries to learn an optimal strategy over time.\n\nIn our context, we can formulate our problem in the reinforcement learning context as follows:\n\n- **State (S)**: A state is defined as a tuple of three elements (i, j, k) where `i` is the current player's score, `j` is the opponent's score, and `k` is the total score accumulated so far in the current turn.\n\n- **Action (A)**: There are two possible actions for the player at any state: 'roll' or 'hold'. \n\n- **Reward (R)**: The reward function is binary. A reward of `1` is given when the player wins (i.e., when the player's score is equal to or more than 100), and `0` otherwise.\n\n- **Policy (π)**: The policy defines the player's behavior at a given state. In this case, the policy is \"hold on 20\", which means the player will hold if the turn total `k` is more than or equal to 20, or if the player's score `i` is greater than or equal to 100.\n\n- **State-Value Function (V)**: The state-value function `V(s)` under a policy `π` is the expected return when starting in state `s` and following `π` thereafter. For us, this will indicate the probability of winning from a given state following our policy.\n\n---\n\n## Modelling the Problem as a Markov Decision Process (MDP)\n\nA Markov Decision Process (MDP) provides a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. It is particularly well suited for this game. We can define our MDP as follows:\n\n- **States (S)** and **Actions (A)**: Defined as above.\n\n- **Transition Probability (P)**: If we denote `s` and `s'` as the initial and subsequent states respectively, and `a` as the action taken, then our transition probability `P(s'|s,a)` is as follows:\n  - If `a = roll`, and the dice result is `d`, then `s' = (i, j, k + d)` with probability `1/6`.\n  - If `a = roll`, and the dice result is `1`, then `s' = (j, i, 0)` with probability `1/6`.\n  - If `a = hold`, then `s' = (j, i + k, 0)` with probability `1`.\n\n- **Reward Function (R)**: As defined above.\n\nThe core of our approach will involve **iterative policy evaluation**, which is a standard method in dynamic programming for estimating the state-value function `V(s)` under a given policy. We will use this method to analyze the \"hold on 20\" strategy. \n\n---\n\n## Implementing Iterative Policy Evaluation\n\nGiven our policy (hold on 20), we want to compute the state-value function `V(s)`, which represents the probability of winning from state `s` under our policy. We initialize all state values to 0, except for terminal states which are set to 1 for winning states and 0 for losing states.\n\nWe then use the Bellman expectation equation for iterative policy evaluation:\n\n$$\nV_{k+1}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} P(s'|s,a) [R(s,a,s') + \\gamma V_k(s')]\n$$\n\nSince our rewards are only at the end of the game, we can simplify the equation to:\n\n$$\nV_{k+1}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} P(s'|s,a) V_k(s')\n$$\n\nHere, `V_k(s)` is the value of state `s` at the `k`-th iteration of the algorithm. `P(s'|s,a)` is the probability of transitioning to state `s'` when action `a` is taken at state `s`. `π(a|s)` represents the probability of taking action `a` at state `s` under policy `π`, which in our case is deterministic.\n\nThis process is repeated until the maximum change in state value function is less than a small threshold, at which point the value function has converged to the true state-value function under the given policy.\n\nTo summarize in 4 steps:\n\n1. **Initialization**: All state values, except for terminal states, are initialized to 0. Terminal states are initialized to 1 if they represent a win and 0 if they represent a loss.\n2. **Policy Evaluation Step**: For each state, we calculate the expected value of each possible action under the current policy. This involves calculating the transition probabilities for each action, and using the current estimates of the state-value function.\n3. **Update Step**: We use the computed values to update our estimates of the state-value function. The largest change in the state-value function across all states is recorded.\n4. **Convergence Check**: The process of steps 2 and 3 is repeated until the largest change in the state-value function is less than a small threshold (delta), indicating that our estimates have converged.\n\nOnce we have implemented the above steps, we can estimate the value of any state for two players following the \"hold on 20\" policy. For example, to estimate the value of the state (50, 50, 10), we perform the following operations:","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Probability of winning in state (i, j, k)\n# We include up to 105 into each state as it's technically possible to reach 105 with this strategy\np = np.zeros((105, 105, 105))\n\n# Example state\nstate = (50, 50, 10) \n\ni, j, k = state\n\n# Terminal states\nif i + k >= 100:\n    p[i, j, k] = 1.0\nelif j >= 100:\n    p[i, j, k] = 0.0\n    \n# Else follow the policy\nelse:\n    \n    # Hold if our current turn total is over 20\n    if k >= 20:\n        \n        # The probability the player wins is equal to the probability the opponent doesn't win\n        p[i, j, k] = 1 - p[j, i + k, 0]\n        \n    # Else we roll\n    else:\n        \n        # Rolling a 1\n        p_roll = (1 - p[j, i, 0]) / 6\n        \n        # Rolling a 2\n        p_roll += p[i, j, k + 2] / 6\n        \n        # Rolling a 3\n        p_roll += p[i, j, k + 3] / 6\n        \n        # Rolling a 4\n        p_roll += p[i, j, k + 4] / 6\n        \n        # Rolling a 5\n        p_roll += p[i, j, k + 5] / 6\n        \n        # Rolling a 6\n        p_roll += p[i, j, k + 6] / 6\n        \n        # Update the current estimate from these values\n        p[i, j, k] = p_roll\n        \nprint(\"Estimate of winning from a single iteration:\", p[i,j,k])","metadata":{"execution":{"iopub.status.busy":"2023-06-17T14:51:54.003772Z","iopub.execute_input":"2023-06-17T14:51:54.004172Z","iopub.status.idle":"2023-06-17T14:51:54.013574Z","shell.execute_reply.started":"2023-06-17T14:51:54.004141Z","shell.execute_reply":"2023-06-17T14:51:54.012563Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"Estimate of winning from a single iteration: 0.16666666666666666\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This process can be repeated for all states to generate the complete state-value function for the \"hold on 20\" policy. Once we have this, we can analyze the effectiveness of the policy and potentially explore improvements or alternatives.\n\n\n#### Improving convergence speed through backward induction\nGiven that our iterative policy evaluation technique relies on bootstrapping from existing estimates, and that initially only the terminal states possess non-zero estimates, we can significantly accelerate the convergence of our method by iterating through the game states in reverse order.\n\nThis method, known as Backward Induction, starts from the maximum possible state, (105, 105, 105) in our case, and moves backwards to the initial state, (0, 0, 0). The primary advantage of this technique is that it allows for the immediate propagation of the value estimates from the terminal states to their predecessors, thereby eliminating the need for the value estimates to slowly propagate forward from the terminal states.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Initialize the state-value function array to zeros\nV = np.zeros((100 + 6, 100 + 6, 100 + 6))\n\n# Small number for stopping condition\ntheta = 1e-5  \ndelta = 1 \n\n# We store all computed max deltas for plotting later\ndelta_list = []\n\nwhile delta > theta:\n    delta = 0\n\n    for i in reversed(range(100 + 6)): # Backward induction\n        for j in reversed(range(100 + 6)):\n            for k in reversed(range(100 + 6)):\n\n                # Copy old value to calculate change later\n                v_old = V[i, j, k]\n\n                # If player's score is >= 100, they've won\n                if i >= 100:\n                    V[i, j, k] = 1.0\n                    continue\n\n                # If opponent's score is >= 100, then player has lost\n                if j >= 100:\n                    V[i, j, k] = 0.0\n                    continue\n\n                # If the turn score is >= 20 or if holding would result in winning the game\n                if k >= 20 or i + k >= 100:\n                    # The new state is (j, i + k, 0) (opponent's turn)\n                    V[i, j, k] = 1 - V[j, min(100, i + k), 0]\n                else:\n                    # The player will roll the dice according to the policy\n                    # Consider all possible outcomes: rolling 1, 2, 3, 4, 5, or 6\n                    v_roll = (1 - V[j, i, 0]) / 6  # Rolling a 1\n                    for roll in range(2, 7):  # Rolling 2, 3, 4, 5, 6\n                        v_roll += V[i, j, k + roll] / 6\n\n                    V[i, j, k] = v_roll\n\n                # Update delta\n                delta = max(delta, abs(v_old - V[i, j, k]))\n                \n    delta_list.append(delta)\n\n    \nimport plotly.graph_objects as go\n\ndef plot_policy_evaluation_convergence(delta_values, title = \"\"):\n    # Create a line plot\n    fig = go.Figure()\n\n    # Add trace\n    fig.add_trace(go.Scatter(\n        y=delta_values,  # errors on y-axis\n        mode='lines+markers',  # line plot with markers\n        marker=dict(\n            size=8,  # size of the markers\n            color='#00CC96',  # color of the markers\n        ),\n        line=dict(\n            width = 3,\n            color='#00CC96',  # color of the line\n        ),\n        hovertemplate='Iterations: %{x}<br>Largest Prob Update: %{y}',  # custom hover template\n        name='Largest Probability Update',  # name of the trace\n    ))\n\n    # Set title and labels\n    fig.update_layout(\n        title='Policy Evaluation ' + title,\n        xaxis_title='Iterations',\n        yaxis_title='Largest Probability Update',\n    )\n\n    # Set y-axis to logarithmic scale\n    fig.update_yaxes(type='log')\n\n    # Show the plot\n    fig.show()\n    \nplot_policy_evaluation_convergence(delta_list, \"- Hold on 20\")","metadata":{"execution":{"iopub.status.busy":"2023-06-17T15:59:39.857670Z","iopub.execute_input":"2023-06-17T15:59:39.858068Z","iopub.status.idle":"2023-06-17T16:00:40.197242Z","shell.execute_reply.started":"2023-06-17T15:59:39.858040Z","shell.execute_reply":"2023-06-17T16:00:40.195713Z"},"trusted":true},"execution_count":111,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"c1eca27f-7d80-4eaf-832c-c4c77e35ab45\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c1eca27f-7d80-4eaf-832c-c4c77e35ab45\")) {                    Plotly.newPlot(                        \"c1eca27f-7d80-4eaf-832c-c4c77e35ab45\",                        [{\"hovertemplate\":\"Iterations: %{x}<br>Largest Prob Update: %{y}\",\"line\":{\"color\":\"#00CC96\",\"width\":3},\"marker\":{\"color\":\"#00CC96\",\"size\":8},\"mode\":\"lines+markers\",\"name\":\"Largest Probability Update\",\"y\":[1.0,0.6397755101084075,0.3944990499232508,0.2595993640775916,0.15986850518101242,0.10597366571618816,0.06475830031098784,0.04338633219590127,0.026240081841115037,0.01771662628180648,0.01154505795266303,0.008068980005172088,0.005372391958606282,0.003485481219374753,0.0022849724508703373,0.0014442358117567888,0.0008845747950240179,0.000526272855716603,0.00030542919253251366,0.00018763165191304498,0.00011671145202818423,0.00007552218259898602,0.00004733281932178812,0.000030453763188331706,0.000019189582236456282,0.000012298170608548986,7.776777091739184e-6],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Policy Evaluation - Hold on 20\"},\"xaxis\":{\"title\":{\"text\":\"Iteration\"}},\"yaxis\":{\"title\":{\"text\":\"Largest Probability Update\"},\"type\":\"log\"}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('c1eca27f-7d80-4eaf-832c-c4c77e35ab45');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]}]}