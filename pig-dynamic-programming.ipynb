{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Solving the Dice Game Pig with Dynamic Programming\n*This notebook is going to be updated frequently, stay tuned*.\n\nWe're going to define the game state as a Markov Process, where each game state is as follows: [player_score, opponent_score, turn_total].","metadata":{}},{"cell_type":"code","source":"import random\n\nclass Pig:\n    def __init__(self):\n        self.state = [0, 0, 0]\n        self.current_player = 1\n        self.player_names = (\"Player 1\", \"Player 2\")\n        \n    def roll(self):\n        dice_value = random.randint(1, 6)\n        if dice_value == 1:\n            self.state[2] = 0\n        else:\n            self.state[2] += dice_value\n        return dice_value\n        \n    def hold(self):\n        self.state[0] += self.state[2]\n        self.state[2] = 0\n        return 0  # 0 indicates hold\n    \n    def change_player(self):\n        self.state[0], self.state[1] = self.state[1], self.state[0]\n        self.current_player = 3 - self.current_player\n    \n    def action(self, action):\n        assert action in [0, 1], \"Invalid move selected.\"  \n        assert not self.is_done(), \"Game is over. Actions not possible.\"\n            \n        if action == 0:\n            roll_value = self.hold()\n        elif action == 1:\n            roll_value = self.roll()\n            \n        info = self.get_info(action, roll_value)\n        \n        if roll_value in [0, 1]:  # if hold or roll a 1\n            if self.state[0] < 100:\n                self.change_player()\n        \n        observations = self.get_observations()\n        reward = self.calculate_reward()\n        done = self.is_done()\n        \n        return observations, reward, done, info\n        \n    def get_observations(self):\n        return self.state.copy()\n    \n    def get_actions(self):\n        return [] if self.is_done() else [0, 1]\n            \n    def reset(self):\n        self.state = [0, 0, 0]\n        self.current_player = 1\n        return self.get_observations()\n    \n    def calculate_reward(self):\n        if self.state[0] >= 100:\n            return 1\n        elif self.state[1] >= 100:\n            return -1\n        else:\n            return 0\n        \n    def get_info(self, action, roll_value):\n        info = self.player_names[self.current_player-1]\n        if action == 0:\n            info += f\" holds, bringing their total score to {self.state[0]}.\"\n        elif action == 1 and roll_value != 1:\n            info += f\" rolls a {roll_value}, bringing their turn score to {self.state[2]} and total score to {self.state[0] + self.state[2]}.\"\n        elif action == 1 and roll_value == 1:\n            info += f\" rolls a 1, losing their turn score, resetting their total score to {self.state[0] + self.state[2]}.\"\n        return info\n        \n    def is_done(self):\n        return self.state[0] >= 100 or self.state[1] >= 100","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:45:34.560436Z","iopub.execute_input":"2023-06-17T10:45:34.560875Z","iopub.status.idle":"2023-06-17T10:45:34.580967Z","shell.execute_reply.started":"2023-06-17T10:45:34.560839Z","shell.execute_reply":"2023-06-17T10:45:34.579978Z"},"trusted":true},"execution_count":291,"outputs":[]}]}